# -*- coding: utf-8 -*-
"""KMNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vVmsKewc0ZahqVjj39xSvZ2GP9OcPr7-

# **Training Dataset**

**Library-library yang digunakan**
"""

# Commented out IPython magic to ensure Python compatibility.
#Primitive function
from __future__ import absolute_import, division, print_function, unicode_literals

#Tensorflow Sensor
try:
#   %tensorflow_version 2.x
except Exception:
  pass

#Tensorflow Library  
import tensorflow as tf
import tensorflow.python.keras
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.models import model_from_json, load_model
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.utils import to_categorical, CustomObjectScope

#Library Lain
import glob
import os
import h5py
import math
import numpy as np
from numpy import genfromtxt
import matplotlib.pyplot as plt
from PIL import Image
import random
import cv2

"""**Import direktori dari Google Drive**"""

from google.colab import drive
drive.mount('/content/drive')

"""**Memasukan Dataset**

***Gambar***

*Dari Numpy Array yang Telah Disave*
"""

data = np.load('/content/drive/My Drive/Colab Notebooks/kmnist-test-imgs.npz')
test_images = data['arr_0']
data = np.load('/content/drive/My Drive/Colab Notebooks/kmnist-test-labels.npz')
test_labels = data['arr_0']
data = np.load('/content/drive/My Drive/Colab Notebooks/kmnist-train-imgs.npz')
train_images = data['arr_0']
data = np.load('/content/drive/My Drive/Colab Notebooks/kmnist-train-labels.npz')
train_labels = data['arr_0']

# Commented out IPython magic to ensure Python compatibility.
#Show All Variabels
# %whos

print(test_images.shape)
print(test_labels.shape)
print(train_images.shape)
print(train_labels.shape)

"""**Menampilkan Gambar**"""

# Plot random Train Images

plt.figure(figsize=(20,20))
for i in range(100):
    plt.subplot(10,10,i+1)
    plt.xticks([])
    plt.yticks([])
    num = random.randint(1,60000)
    plt.grid(False)
    plt.imshow(train_images[num], cmap="Greys")
    plt.xlabel(train_labels[num])
plt.show()

#Plot random Test Images

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    num = random.randint(1,10000)
    plt.grid(False)
    plt.imshow(test_images[num], cmap="Greys")
    plt.xlabel(test_labels[num])
plt.show()

"""**Sistem Learning**"""

# Reshape for showing image at the above
test_images = test_images.reshape(test_images.shape[0], 28, 28)
train_images = train_images.reshape(train_images.shape[0], 28, 28)

#Using Resolution 28x28 pixels

#Reshape for training
test_images = test_images.reshape(test_images.shape[0], 28, 28,1)
train_images = train_images.reshape(train_images.shape[0], 28, 28,1)

#Models : CNN + Maxpooling + CNN + Maxpool + Flatten + NN + NN
model = models.Sequential()
model.add(layers.Conv2D(6, (3, 3),use_bias=False, activation='relu', input_shape=(28, 28, 1), name ='layer_1'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(6, (3, 3),use_bias=False, activation='relu', name = 'layer_2'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(64, use_bias=False, activation='relu', name='layer_3'))
model.add(layers.Dense(10, use_bias=False, activation='softmax', name='layer_Output'))

#Showing the Summary
model.summary()

#Compilation & Training
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_images, train_labels, epochs =3,
                    validation_data=(test_images,test_labels))

"""**Summary Hasil Training**"""

#Plot accuracy
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

#Plot All
fig = plt.figure()
plt.subplot(2,1,1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')

plt.subplot(2,1,2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')

plt.tight_layout()

#Print the Evaluation Variabels
print(test_acc)
print(test_loss)

"""# **Pembuatan File Weights**

**Simpan Model ke File h5 dan json**
"""

#Serialize model to JSON
model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
#Serialize weights to HDF5
model.save_weights("CNN_Model.h5")
print("Saved model to disk")

"""# **Verifikasi**"""

#Open File
json_file = open('/content/model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
#Load weights into new model
loaded_model.load_weights("/content/CNN_Model.h5")
print("Loaded model from disk")

#List all the h5 directory
f = h5py.File("/content/CNN_Model.h5")
list(f)
list(f['layer_1'])

#Get weight to local variabels
with h5py.File('/content/CNN_Model.h5','r') as hdf:
    layer_1_data = np.array(hdf.get('layer_1/layer_1/kernel:0'))
    layer_2_data = np.array(hdf.get('layer_2/layer_2/kernel:0'))
    layer_3_data = np.array(hdf.get('layer_3/layer_3/kernel:0'))
    layer_out_data = np.array(hdf.get('layer_Output/layer_Output/kernel:0'))

"""**Verifikasi Weight**"""

#Print all shape weights
print(layer_1_data.shape)
print(layer_2_data.shape)
print(layer_3_data.shape)
print(layer_out_data.shape)

#Verification
weights = np.array(model.get_weights())
print(np.array_equal(weights[0],layer_1_data))
print(np.array_equal(weights[1],layer_2_data))
print(np.array_equal(weights[2],layer_3_data))
print(np.array_equal(weights[3],layer_out_data))

"""# **Kuantisasi Weights**

**Buka File h5 dan json**
"""

# Model reconstruction from JSON file
with open('/content/model.json', 'r') as f:
    loaded_model = model_from_json(f.read())

# Load weights into the new model
loaded_model.load_weights('/content/CNN_Model.h5')
loaded_model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")
with h5py.File('/content/CNN_Model.h5','r') as hdf:
    layer_1_data = np.array(hdf.get('layer_1/layer_1/kernel:0'))
    layer_2_data = np.array(hdf.get('layer_2/layer_2/kernel:0'))
    layer_3_data = np.array(hdf.get('layer_3/layer_3/kernel:0'))
    layer_out_data = np.array(hdf.get('layer_Output/layer_Output/kernel:0'))
    
#load dataset SUDAH

"""**Verifikasi kedua**"""

#Print all shape data weights
print(layer_1_data.shape)
print(layer_2_data.shape)
print(layer_3_data.shape)
print(layer_out_data.shape)

"""**Fungsi-fungsi yang digunakan**"""

def pool2d(mat,ksize,method='max',pad=False):
    m, n = mat.shape[1:]
    ky,kx=ksize

    _ceil=lambda x,y: int(numpy.ceil(x/float(y)))

    if pad:
        ny=_ceil(m,ky)
        nx=_ceil(n,kx)
        size=(mat.shape[0],ny*ky, nx*kx)
        mat_pad=numpy.full(size,numpy.nan)
        mat_pad[:m,:n,...]=mat
    else:
        ny=m//ky
        nx=n//kx
        mat_pad=mat[..., :ny*ky, :nx*kx]

    new_shape=(mat.shape[0],ny,ky,nx,kx)

    if method=='max':
        result=np.nanmax(mat_pad.reshape(new_shape),axis=(2,4))
    else:
        result=np.nanmean(mat_pad.reshape(new_shape),axis=(2,4))

    return result

def channel_im2col(A):
    B = [3,3]
    skip=[1,1]
    # Parameters 
    D,M,N = A.shape
    col_extent = N - B[1] + 1
    row_extent = M - B[0] + 1

    # Get Starting block indices
    start_idx = np.arange(B[0])[:,None]*N + np.arange(B[1])

    # Generate Depth indeces
    didx=M*N*np.arange(D)
    start_idx=(didx[:,None]+start_idx.ravel()).reshape((-1,B[0],B[1]))

    # Get offsetted indices across the height and width of input array
    offset_idx = np.arange(row_extent)[:,None]*N + np.arange(col_extent)

    # Get all actual indices & index into input array for final output
    out = np.take (A,start_idx.ravel()[:,None] + offset_idx[::skip[0],::skip[1]].ravel())
    return out


def conv2d(filt,act):
    '''
    conv 2d function
    act = widht, height,depth (w1,h1,d1)
    default = 3,3 filter, stride = 1, p = 0
    im2col reshape
    
    W2=(W1−F+2P)/S+1
    H2=(H1−F+2P)/S+1
    D2=K
    '''
    P = 0
    S = 1
    D1,H1,W1 = act.shape
    n_f,d_f,h_f,w_f = filt.shape
    window_size = d_f*h_f*w_f
    filter_reshape = n_f,window_size
    W2=(W1-h_f +2*P)//S+1
    H2=(H1- h_f+2*P)//S+1
    D2= n_f
    m = channel_im2col(act)
    filt = filt.reshape(filter_reshape)
    res = np.matmul(filt, m)
    res = res.reshape(D2,W2,H2)
    return res

def softmax(X):
    expo = np.exp(X)
    expo_sum = np.sum(np.exp(X))
    return expo/expo_sum

def relu(X):
    return np.maximum(0,X)

def prequant(data,bits):
    MaxValue = np.amax(data)
    MinValue = np.amin(data)
    Range_Real = MaxValue - MinValue
    if(Range_Real==0):
        Range_Real = 1
    Scale = (Range_Real/(pow(2,bits)-1))
    return Scale, Range_Real

def quantize(Val, Range_Real, Scale, zero_point):  
    temp = round(Val * (1/Scale) + zero_point)
    return temp
quantizefunc = np.vectorize(quantize)

def Model2(img):
    global q_weight1
    global q_weight2
    global q_weight3
    # global q_weightout
    Scale_w1,Range_Real_w1 = prequant(layer_1_data,8)
    Scale_d1,Range_Real_d1 = prequant(img.reshape(1,28,28),8)
    quantized_w1 = quantizefunc(layer_1_data,Range_Real_w1,Scale_w1,0)
    quantized_d1 = quantizefunc(img.reshape(1,28,28),Range_Real_d1,Scale_d1,0)
    q_weight1 = quantized_w1

    layer_1_out = relu(conv2d(quantized_w1,quantized_d1))
    layer_1_out = Scale_d1*Scale_w1*pool2d(layer_1_out, (2, 2))
    
    Scale_w2,Range_Real_w2 = prequant(layer_2_data,8)
    Scale_d2,Range_Real_d2 = prequant(layer_1_out,8)
    quantized_w2 = quantizefunc(layer_2_data,Range_Real_w2,Scale_w2,0)
    quantized_d2 = quantizefunc(layer_1_out,Range_Real_d2,Scale_d2,0)
    q_weight2 = quantized_w2
    
    layer_2_out = relu(conv2d(quantized_w2,quantized_d2))
    layer_2_out = Scale_d2*Scale_w2*pool2d(layer_2_out, (2, 2))
    layer_2_out = layer_2_out.transpose(1,2,0) ### ubah format data dari CHW ke WHC
    layer_2_out = layer_2_out.reshape(150)
    
    Scale_w3,Range_Real_w3 = prequant(layer_3_data,8)
    Scale_d3,Range_Real_d3 = prequant(layer_2_out,8)
    quantized_w3 = quantizefunc(layer_3_data,Range_Real_w3,Scale_w3,0)
    quantized_d3 = quantizefunc(layer_2_out,Range_Real_d3,Scale_d3,0)
    q_weight3 = quantized_w3

    layer_3_out = np.matmul(quantized_d3,quantized_w3)
    layer_3_out = Scale_d3*Scale_w3*relu(layer_3_out)
    
    Scale_w4,Range_Real_w4 = prequant(layer_out_data,8)
    Scale_d4,Range_Real_d4 = prequant(layer_3_out,8)
    quantized_w4 = quantizefunc(layer_out_data,Range_Real_w4,Scale_w4,0)
    quantized_d4 = quantizefunc(layer_3_out,Range_Real_d4,Scale_d4,0)
    q_weightout = quantized_w4
    
    layer_last_out = Scale_d4*Scale_w4*np.matmul(quantized_d4,quantized_w4)
    layer_last_out = softmax(layer_last_out)
    return layer_last_out,

#Transpose Layer 1 dan Layer 2
layer_1_data = layer_1_data.transpose(3,2,0,1)
layer_2_data= layer_2_data.transpose(3,2,0,1)

"""**Training & Verifikasi dari weights yang telah ada**"""

#Train from the weights

val_num_uq = 0
val_num_q = 0
correct_q = np.empty(10000)
correct_uq = np.empty(10000)
val_num_q_2 = 0
correct_q_2 = np.empty(10000)

for i in range(10000):
    indata = test_images[i]/255
    label = test_labels[i]
    
    #data = indata.ravel()
    
    keras_res = loaded_model.predict_classes(indata.reshape(1,28,28,1))
    res = Model2(indata.reshape(1,28,28))
    
    if(label==keras_res[0]):        
        correct_q[val_num_q] = i
        val_num_q += 1

    if(label==np.argmax(res)):
        correct_uq[val_num_uq] = i
        val_num_uq += 1
        
print('val_num_keras_model :', val_num_q)
print('val_num_quantized_model :', val_num_uq)

print('val val_num_keras_model :', (val_num_q/10000)*100,'%')
print('val quantized  :', (val_num_uq/10000)*100,'%')

"""**Training & Verifikasi dari test images**"""

timg3=cv2.imread("/content/3.jpg",cv2.IMREAD_GRAYSCALE)
timg3 =cv2.bitwise_not(timg3)
timg3 = cv2.resize(timg3, (28, 28))

timg2=cv2.imread("/content/2.jpg",cv2.IMREAD_GRAYSCALE)
timg2 =cv2.bitwise_not(timg2)
timg2 = cv2.resize(timg2, (28, 28))

timg5=cv2.imread("/content/5.jpg",cv2.IMREAD_GRAYSCALE)
timg5 =cv2.bitwise_not(timg5)
timg5 = cv2.resize(timg5, (28, 28))

timg7=cv2.imread("/content/7.jpg",cv2.IMREAD_GRAYSCALE)
timg7 =cv2.bitwise_not(timg7)
timg7 = cv2.resize(timg7, (28, 28))

#Train from file Label 2

indata = timg2/255
res = Model2(indata.reshape(1,28,28))
plt.imshow(timg2)
print("Gambar ini diklasifikasikan sebagai :")
print(np.argmax(res))

#Train from file Label 3

indata = timg3/255
res = Model2(indata.reshape(1,28,28))
plt.imshow(timg3)
print("Gambar ini diklasifikasikan sebagai :")
print(np.argmax(res))

#Train from file Label 5

indata = timg5/255
res = Model2(indata.reshape(1,28,28))
plt.imshow(timg5)
print("Gambar ini diklasifikasikan sebagai :")
print(np.argmax(res))

#Train from file Label 7

indata = timg7/255
res = Model2(indata.reshape(1,28,28))
plt.imshow(timg7)
print("Gambar ini diklasifikasikan sebagai :")
print(np.argmax(res))

#Print all shape weights
print(q_weight1.shape)
print(q_weight2.shape)
print(q_weight3.shape)
print(q_weightout.shape)

#Data Weights sebelum kuantisasi
print("Weights Layer 1 :\n")
print(layer_1_data)
print(np.max(layer_1_data))

#Weights sesudah kuantisasi
print("Weights Layer 1 :\n")
print(q_weight1)
print(np.max(q_weight1))

print("Weights Layer 2 :\n")
print(q_weight2[1])

#Output to .pny file (numpy array)
np.save('q_weight1.npy', q_weight1)
np.save('q_weight2.npy', q_weight2)
np.save('q_weight3.npy', q_weight3)
np.save('q_weightout.npy', q_weightout)
print("DONE")